\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Rclean: A Tool for Writing Cleaner, More Transparent Code},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{{#1}}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\title{Rclean: A Tool for Writing Cleaner, More Transparent Code}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

\section{Introduction}\label{introduction}

The growth of programming in the sciences has been explosive in the last
decade. This has facilitated the rapid advancement of science through
the agile development of computational tools. However, concerns have
begun to surface about the reproducibility of scientific research in
general (R. D. Peng et al. 2011 Baker (2016)) and the potential issues
stemming from issues with analytical software (Pasquier et al. 2017
Stodden, Seiler, and Ma (2018)). Specifically, there is a growing
recognition across disciplines that simply making data and software
``available'' is not enough and that there is a need to improve the
transparency and stability of scientific software (Pasquier et al.
2018).

At the core of the growth of scientific computation, the \texttt{R}
statistical programming language has grown exponentially to become one
of the top ten programming languages in use today. At it's root R is a
\emph{statistical} programming language. That is, it was designed for
use in analytical workflows; and the majority of the R community is
focused on producing code for idiosyncratic projects that are
\emph{results} oriented. Also, R's design is intentionally at a level
that abstracts many aspects of programming that would otherwise act as a
barrier to entry for many users. This is good in that there are many
people who use R to their benefit with little to no formal training in
computer science or software engineering, but these same users can also
be frequently frustrated by code that is fragile, buggy and complicated
enough to quickly become obtuse even to the authors. The stability,
reproducibility and re-use of scientific analyses in R would be improved
by refactoring, which is a common practice in software engineering
({\textbf{???}}). From this perspective, tools that can lower the time
and energy required to refactor analytical scripts and otherwise help to
``clean'' code, but abstracted enough to be easily accessible, could
have a significant impact on scientific reproducibility across all
disciplines (Visser et al. 2015).

To provide support for easier refactoring in R, we have created
\texttt{Rclean}. The \texttt{Rclean} package provides tools to
automatically reduce a script to the parts that are specifically
relevant to a research product (e.g. a scientific report, academic talk,
research article, etc.). Although potentially useful to all R coders, it
was designed to ease refactoring for scientists that use R but do not
have formal training in software engineering. Here, we detail the
structure of the package's API, describe the general workflow
illustrated by an example use case and provide some background on how
data provenance enables the underlying functionality of the package. We
then end with a discussion of future applications of data provenance in
the context of ``code cleaning'' and the potential integration with
other software engineering tools for the R community.

\section{Methods}\label{methods}

More often then not, when someone is writing an R script, the intent to
produce a set of results, such as a statistical analysis, figure, table,
etc. This set of results is always a subset of a much larger set of
possible ways to explore a dataset, as there are many statistical
approaches and tests, let alone ways to create visualizations and other
representations of patterns in data. This commonly leads to lengthy,
complicated scripts from which researchers manually subset results, but
never refactor, i.e.~refine code so that it is shorter and focused on a
desired product.

The goal of \texttt{Rclean} is to provide a set of tools that help
someone reduce and organize code based on results. The package uses an
automated technique based on data provenance (details below) to analyze
existing scripts and provide ways to identify and extract code to
produce a desired output. To keep the process simple and
straight-forward much of this process has been abstracted for the user,
and the API has been kept to a minimum set of functions that enable a
user to conduct the following basic workflow:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Obtain the ``cleaned'' code for a result(s).
\item
  Transfer the code to a new context (e.g.~a new script, function,
  reproducible example, web-app, etc.).
\item
  Get information about the possible results and repeat as needed.
\end{enumerate}

\subsection{The API}\label{the-api}

The package's main functions are \texttt{clean} and \texttt{keep}. When
provided a file path to a script and the name of a result (or a set of
results), \texttt{clean} analyzes the script's code and extracts the
lines of code required to produce the results. By default, code is
formatted following general best practices recommended by the Tidyverse
via the \texttt{styler} package. This code can then be passed to the
\texttt{keep} function, which can either write the code to disk or copy
the code to the user's clipboard (if no output file path is supplied)
and the user can paste the code into another location (e.g.~a script
editor).

In the process of cleaning a script, it is likely that a user will not
know the precise objects they want and might require some help analyzing
it. There are several functions to help with this process. The
\texttt{get\_vars} function will return a list of possible results for a
given script at a supplied file path. This is obviously an important
step, and justifiably, the default behavior of the \texttt{clean}
function is to run \texttt{get\_vars} if no results are supplied. To
help with limiting and checking the selection of results, the
\texttt{code\_graph} function creates a network graph of the
relationships among the various results and lines of code in the script,
which is the function used to create the figure demonstrating data
provenance \ref{prov-graph}. Last, the \texttt{get\_libs} function can
be used to detect the packages that a given script depends on, which it
will return as coded library calls that can be inserted into a cleaned
script.

\subsection{Data Provenance}\label{data-provenance}

All of these processes rely on the generation of data provenance. The
term provenance means information about the origins of some object. Data
provenance is a formal representation of the execution of a
computational process (\url{https://www.w3.org/TR/prov-dm/}), to
rigorously determine the the unique computational pathway from inputs to
results (Carata et al. 2014). To avoid confusion, note that ``data'' in
this context is used in a broad sense to include all of the information
generated during computation, not just the data that are collected in a
research project that are used as input to an analysis. Having the
formalized, mathematically rigorous representation that data provenance
provides guarantees that the analyses that \texttt{Rclean} conducts are
theoretically sound. Most importantly, because the relationships defined
by the provenance can be represented as a graph, it is possible to apply
network search algorithms to determine the minimum and sufficient code
needed to generate the chosen result in the \texttt{clean} function.

There are multiple approaches to collecting data provenance, but
\texttt{Rclean} uses ``prospective'' provenance, which analyzes code and
uses language specific information to predict the relationship among
processes and data objects. \texttt{Rclean} relies on a library called
\texttt{CodeDepends} to gather the prospective provenance for each
script. For more information on the mechanics of the
\texttt{CodeDepends} package, see ({\textbf{???}}). To get an idea of
what data provenance is, take a look at the \texttt{code\_graph}
function. The plot that it generates is a graphical representation of
the prospective provenance generated for \texttt{Rclean}
\ref{prov-graph}.

```\{R prov-graph, fig.cap = ``Network diagram of the prospective data
provenance generated for an example script. Arrows indicate which lines
of code (numbered) produced which objects (named).'', echo = FALSE\}

script \textless{}- system.file( ``example'', ``simple\_script.R'',
package = ``Rclean'') code\_graph(script)

\begin{verbatim}

Relatedly, it is important to point out that `Rclean` *does not* keep
comments present in code. This could be seen as a limitation of the
data provenance, which currently does not assign them a
relationships. Therefore, although there is often very useful or even
invaluable information in comments, the `clean` function removes
them. This is a general issue with automated methods for detecting the
relationships between comments and the code itself. Comments at the
end of lines are typically relevant to the line they are on, but this
is not a requirement and could refer to other lines. Also, comments
occupying their own lines usually refer to the following lines, but
this is also not necessarily the case. As `clean` depends on the
unambiguous determination of relationships in the production of
results, it cannot operate automatically on comments. However,
comments in the original code remain untouched and can be used to
inform the reduced code. Also, as the `clean` function is oriented
toward isolating code based on a specific result, the resulting code
tends to naturally support the generation of new comments that are
higher level (e.g. "The following produces a plot of the mean response
of each treatment group."), and lower level comments are not necessary
because the code is simpler and clearer.

In the future, it would also be useful to extend the existing
framework to support other provenance methods. One such possibility is
*retrospective provenance*, which tracks a computational process as it
is executing. Through this active, concurrent monitoring,
retrospective provenance can gather information that static
prospective provenance can't. Greater details of the computational
process would enable other features that could address some
challenges, such as processing information from comments, parsing
control statements and replicating random processes. However, using
retrospective provenance comes at a cost. In order to gather it, the
script needs to be executed. When scripts are computationally
intensive or contain bugs that stop execution, then retrospective
provenance can not be obtained for part or all of the code. Some work
has already been done in the direction of implementing retrospective
provenance for code cleaning in R (see http://end-to-end-provenance.github.io).


# Results

## Example

Data analysis can be messy and complicated, so it's no wonder that the
code often reflects this. "What did I measure? What analyses are
relevant to them? Do I need to transform the data? What's the function
for the analysis I want to run?" This is why having a way to isolate
code based on variables can be valuable. The following is an example
of a script that has some complications. As you can see, although the
script is not extremely long or complicated, it's difficult enough to
make it frustrating to visualize it in its entirety and pick through
it.



\end{verbatim}

\subsection{\texorpdfstring{{[}1{]}
``library(stats)''}{{[}1{]} library(stats)}}\label{librarystats}

\subsection{\texorpdfstring{{[}2{]} ``x \textless{}-
1:100''}{{[}2{]} x \textless{}- 1:100}}\label{x---1100}

\subsection{\texorpdfstring{{[}3{]} ``x \textless{}-
log(x)''}{{[}3{]} x \textless{}- log(x)}}\label{x---logx}

\subsection{\texorpdfstring{{[}4{]} ``x \textless{}- x *
2''}{{[}4{]} x \textless{}- x * 2}}\label{x---x-2}

\subsection{\texorpdfstring{{[}5{]} ``x \textless{}- lapply(x, rep,
times =
4)''}{{[}5{]} x \textless{}- lapply(x, rep, times = 4)}}\label{x---lapplyx-rep-times-4}

\subsection{\texorpdfstring{{[}6{]} ``\#\#\# This is a note that I made
for
myself.''}{{[}6{]} \#\#\# This is a note that I made for myself.}}\label{this-is-a-note-that-i-made-for-myself.}

\subsection{\texorpdfstring{{[}7{]} ``\#\#\# Next time, make sure to use
a different
analysis.''}{{[}7{]} \#\#\# Next time, make sure to use a different analysis.}}\label{next-time-make-sure-to-use-a-different-analysis.}

\subsection{\texorpdfstring{{[}8{]} ``\#\#\# Also, check with someone
about how to run some other
analysis.''}{{[}8{]} \#\#\# Also, check with someone about how to run some other analysis.}}\label{also-check-with-someone-about-how-to-run-some-other-analysis.}

\subsection{\texorpdfstring{{[}9{]} ``x \textless{}- do.call(cbind,
x)''}{{[}9{]} x \textless{}- do.call(cbind, x)}}\label{x---do.callcbind-x}

\subsection{\texorpdfstring{{[}10{]} ``''}{{[}10{]} }}\label{section}

\subsection{\texorpdfstring{{[}11{]} ``\#\#\# Now I'm going to create a
different
variable.''}{{[}11{]} \#\#\# Now I'm going to create a different variable.}}\label{now-im-going-to-create-a-different-variable.}

\subsection{\texorpdfstring{{[}12{]} ``\#\#\# This is the best variable
the world has ever
seen.''}{{[}12{]} \#\#\# This is the best variable the world has ever seen.}}\label{this-is-the-best-variable-the-world-has-ever-seen.}

\subsection{\texorpdfstring{{[}13{]} ``''}{{[}13{]} }}\label{section-1}

\subsection{\texorpdfstring{{[}14{]} ``x2 \textless{}- sample(10:1000,
100)''}{{[}14{]} x2 \textless{}- sample(10:1000, 100)}}\label{x2---sample101000-100}

\subsection{\texorpdfstring{{[}15{]} ``x2 \textless{}- lapply(x2,
rnorm)''}{{[}15{]} x2 \textless{}- lapply(x2, rnorm)}}\label{x2---lapplyx2-rnorm}

\subsection{\texorpdfstring{{[}16{]} ``''}{{[}16{]} }}\label{section-2}

\subsection{\texorpdfstring{{[}17{]} ``\#\#\# Wait, now I had another
thought about x that I want to work
through.''}{{[}17{]} \#\#\# Wait, now I had another thought about x that I want to work through.}}\label{wait-now-i-had-another-thought-about-x-that-i-want-to-work-through.}

\subsection{\texorpdfstring{{[}18{]} ``''}{{[}18{]} }}\label{section-3}

\subsection{\texorpdfstring{{[}19{]} ``x \textless{}- x *
2''}{{[}19{]} x \textless{}- x * 2}}\label{x---x-2-1}

\subsection{\texorpdfstring{{[}20{]} ``colnames(x) \textless{}-
paste0("X",
seq\_len(ncol(x)))''}{{[}20{]} colnames(x) \textless{}- paste0("X", seq\_len(ncol(x)))}}\label{colnamesx---paste0x-seq_lenncolx}

\subsection{\texorpdfstring{{[}21{]} ``rownames(x) \textless{}-
LETTERS{[}seq\_len(nrow(x)){]}''}{{[}21{]} rownames(x) \textless{}- LETTERS{[}seq\_len(nrow(x)){]}}}\label{rownamesx---lettersseq_lennrowx}

\subsection{\texorpdfstring{{[}22{]} ``x \textless{}-
t(x)''}{{[}22{]} x \textless{}- t(x)}}\label{x---tx}

\subsection{\texorpdfstring{{[}23{]} ``x{[}, "A"{]} \textless{}-
sqrt(x{[},
"A"{]})''}{{[}23{]} x{[}, "A"{]} \textless{}- sqrt(x{[}, "A"{]})}}\label{x-a---sqrtx-a}

\subsection{\texorpdfstring{{[}24{]} ``''}{{[}24{]} }}\label{section-4}

\subsection{\texorpdfstring{{[}25{]} ``for (i in
seq\_along(colnames(x)))
\{''}{{[}25{]} for (i in seq\_along(colnames(x))) \{}}\label{for-i-in-seq_alongcolnamesx}

\subsection{{[}26{]} " set.seed(17)"}\label{set.seed17}

\subsection{{[}27{]} " x{[}, i{]} \textless{}- x{[}, i{]} +
runif(length(x{[}, i{]}), -1, 1)"}\label{x-i---x-i-runiflengthx-i--1-1}

\subsection{\texorpdfstring{{[}28{]}
``\}''}{{[}28{]} \}}}\label{section-5}

\subsection{\texorpdfstring{{[}29{]} ``''}{{[}29{]} }}\label{section-6}

\subsection{\texorpdfstring{{[}30{]} ``\#\#\# Ok. Now I can get back to
x2.''}{{[}30{]} \#\#\# Ok. Now I can get back to x2.}}\label{ok.-now-i-can-get-back-to-x2.}

\subsection{\texorpdfstring{{[}31{]} ``\#\#\# Now I just need to check
out a bunch of stuff with
it.''}{{[}31{]} \#\#\# Now I just need to check out a bunch of stuff with it.}}\label{now-i-just-need-to-check-out-a-bunch-of-stuff-with-it.}

\subsection{\texorpdfstring{{[}32{]} ``''}{{[}32{]} }}\label{section-7}

\subsection{\texorpdfstring{{[}33{]} ``lapply(x2,
length){[}1{]}''}{{[}33{]} lapply(x2, length){[}1{]}}}\label{lapplyx2-length1}

\subsection{\texorpdfstring{{[}34{]} ``max(unlist(lapply(x2,
length)))''}{{[}34{]} max(unlist(lapply(x2, length)))}}\label{maxunlistlapplyx2-length}

\subsection{\texorpdfstring{{[}35{]} ``range(unlist(lapply(x2,
length)))''}{{[}35{]} range(unlist(lapply(x2, length)))}}\label{rangeunlistlapplyx2-length}

\subsection{\texorpdfstring{{[}36{]}
``head(x2{[}{[}1{]}{]})''}{{[}36{]} head(x2{[}{[}1{]}{]})}}\label{headx21}

\subsection{\texorpdfstring{{[}37{]}
``tail(x2{[}{[}1{]}{]})''}{{[}37{]} tail(x2{[}{[}1{]}{]})}}\label{tailx21}

\subsection{\texorpdfstring{{[}38{]} ``''}{{[}38{]} }}\label{section-8}

\subsection{\texorpdfstring{{[}39{]} ``\#\# Now, based on that stuff, I
need to subset
x2.''}{{[}39{]} \#\# Now, based on that stuff, I need to subset x2.}}\label{now-based-on-that-stuff-i-need-to-subset-x2.}

\subsection{\texorpdfstring{{[}40{]} ``''}{{[}40{]} }}\label{section-9}

\subsection{\texorpdfstring{{[}41{]} ``x2 \textless{}- lapply(x2,
function(x)
x{[}1:10{]})''}{{[}41{]} x2 \textless{}- lapply(x2, function(x) x{[}1:10{]})}}\label{x2---lapplyx2-functionx-x110}

\subsection{\texorpdfstring{{[}42{]} ``''}{{[}42{]} }}\label{section-10}

\subsection{\texorpdfstring{{[}43{]} ``\#\# And turn it into a
matrix.''}{{[}43{]} \#\# And turn it into a matrix.}}\label{and-turn-it-into-a-matrix.}

\subsection{\texorpdfstring{{[}44{]} ``x2 \textless{}- do.call(rbind,
x2)''}{{[}44{]} x2 \textless{}- do.call(rbind, x2)}}\label{x2---do.callrbind-x2}

\subsection{\texorpdfstring{{[}45{]} ``''}{{[}45{]} }}\label{section-11}

\subsection{\texorpdfstring{{[}46{]} ``\#\# Now, based on x2, I need to
create
x3.''}{{[}46{]} \#\# Now, based on x2, I need to create x3.}}\label{now-based-on-x2-i-need-to-create-x3.}

\subsection{\texorpdfstring{{[}47{]} ``x3 \textless{}- x2{[},
1:2{]}''}{{[}47{]} x3 \textless{}- x2{[}, 1:2{]}}}\label{x3---x2-12}

\subsection{\texorpdfstring{{[}48{]} ``x3 \textless{}- apply(x3, 2,
round, digits =
3)''}{{[}48{]} x3 \textless{}- apply(x3, 2, round, digits = 3)}}\label{x3---applyx3-2-round-digits-3}

\subsection{\texorpdfstring{{[}49{]} ``''}{{[}49{]} }}\label{section-12}

\subsection{\texorpdfstring{{[}50{]} ``\#\# Oh wait! Another thought
about
x.''}{{[}50{]} \#\# Oh wait! Another thought about x.}}\label{oh-wait-another-thought-about-x.}

\subsection{\texorpdfstring{{[}51{]} ``''}{{[}51{]} }}\label{section-13}

\subsection{\texorpdfstring{{[}52{]} ``x{[}, 1{]} \textless{}- x{[},
1{]} * 2 +
10''}{{[}52{]} x{[}, 1{]} \textless{}- x{[}, 1{]} * 2 + 10}}\label{x-1---x-1-2-10}

\subsection{\texorpdfstring{{[}53{]} ``x{[}, 2{]} \textless{}- x{[},
1{]} + x{[},
2{]}''}{{[}53{]} x{[}, 2{]} \textless{}- x{[}, 1{]} + x{[}, 2{]}}}\label{x-2---x-1-x-2}

\subsection{\texorpdfstring{{[}54{]} ``x{[}, "A"{]} \textless{}- x{[},
"A"{]} *
2''}{{[}54{]} x{[}, "A"{]} \textless{}- x{[}, "A"{]} * 2}}\label{x-a---x-a-2}

\subsection{\texorpdfstring{{[}55{]} ``''}{{[}55{]} }}\label{section-14}

\subsection{\texorpdfstring{{[}56{]} ``\#\# Now, I want to run an
analysis on two variables in x2 and
x3.''}{{[}56{]} \#\# Now, I want to run an analysis on two variables in x2 and x3.}}\label{now-i-want-to-run-an-analysis-on-two-variables-in-x2-and-x3.}

\subsection{\texorpdfstring{{[}57{]} ``''}{{[}57{]} }}\label{section-15}

\subsection{\texorpdfstring{{[}58{]} ``fit.23 \textless{}- lm(x2
\textasciitilde{} x3, data = data.frame(x2{[}, 1{]}, x3{[},
1{]}))''}{{[}58{]} fit.23 \textless{}- lm(x2 \textasciitilde{} x3, data = data.frame(x2{[}, 1{]}, x3{[}, 1{]}))}}\label{fit.23---lmx2-x3-data-data.framex2-1-x3-1}

\subsection{\texorpdfstring{{[}59{]}
``summary(fit.23)''}{{[}59{]} summary(fit.23)}}\label{summaryfit.23}

\subsection{\texorpdfstring{{[}60{]} ``''}{{[}60{]} }}\label{section-16}

\subsection{\texorpdfstring{{[}61{]} ``\#\# And while I'm at it, I
should do an analysis on
x.''}{{[}61{]} \#\# And while I'm at it, I should do an analysis on x.}}\label{and-while-im-at-it-i-should-do-an-analysis-on-x.}

\subsection{\texorpdfstring{{[}62{]} ``''}{{[}62{]} }}\label{section-17}

\subsection{\texorpdfstring{{[}63{]} ``x \textless{}-
data.frame(x)''}{{[}63{]} x \textless{}- data.frame(x)}}\label{x---data.framex}

\subsection{\texorpdfstring{{[}64{]} ``fit.xx \textless{}-
lm(A\textasciitilde{}B, data =
x)''}{{[}64{]} fit.xx \textless{}- lm(A\textasciitilde{}B, data = x)}}\label{fit.xx---lmab-data-x}

\subsection{\texorpdfstring{{[}65{]}
``summary(fit.xx)''}{{[}65{]} summary(fit.xx)}}\label{summaryfit.xx}

\subsection{\texorpdfstring{{[}66{]}
``shapiro.test(residuals(fit.xx))''}{{[}66{]} shapiro.test(residuals(fit.xx))}}\label{shapiro.testresidualsfit.xx}

\subsection{\texorpdfstring{{[}67{]} ``''}{{[}67{]} }}\label{section-18}

\subsection{\texorpdfstring{{[}68{]} ``\#\# Ah, it looks like I should
probably transform
A.''}{{[}68{]} \#\# Ah, it looks like I should probably transform A.}}\label{ah-it-looks-like-i-should-probably-transform-a.}

\subsection{\texorpdfstring{{[}69{]} ``\#\# Let's try
that.''}{{[}69{]} \#\# Let's try that.}}\label{lets-try-that.}

\subsection{\texorpdfstring{{[}70{]} ``fit\_sqrt\_A \textless{}-
lm(I(sqrt(A))\textasciitilde{}B, data =
x)''}{{[}70{]} fit\_sqrt\_A \textless{}- lm(I(sqrt(A))\textasciitilde{}B, data = x)}}\label{fit_sqrt_a---lmisqrtab-data-x}

\subsection{\texorpdfstring{{[}71{]}
``summary(fit\_sqrt\_A)''}{{[}71{]} summary(fit\_sqrt\_A)}}\label{summaryfit_sqrt_a}

\subsection{\texorpdfstring{{[}72{]}
``shapiro.test(residuals(fit\_sqrt\_A))''}{{[}72{]} shapiro.test(residuals(fit\_sqrt\_A))}}\label{shapiro.testresidualsfit_sqrt_a}

\subsection{\texorpdfstring{{[}73{]} ``''}{{[}73{]} }}\label{section-19}

\subsection{\texorpdfstring{{[}74{]} ``\#\# Looks
good!''}{{[}74{]} \#\# Looks good!}}\label{looks-good}

\subsection{\texorpdfstring{{[}75{]} ``''}{{[}75{]} }}\label{section-20}

\subsection{\texorpdfstring{{[}76{]} ``\#\# After that. I came back and
ran another analysis
with''}{{[}76{]} \#\# After that. I came back and ran another analysis with}}\label{after-that.-i-came-back-and-ran-another-analysis-with}

\subsection{\texorpdfstring{{[}77{]} ``\#\# x2 and a new
variable.''}{{[}77{]} \#\# x2 and a new variable.}}\label{x2-and-a-new-variable.}

\subsection{\texorpdfstring{{[}78{]} ``''}{{[}78{]} }}\label{section-21}

\subsection{\texorpdfstring{{[}79{]} ``z \textless{}- c(rep("A",
nrow(x2) / 2), rep("B", nrow(x2) /
2))''}{{[}79{]} z \textless{}- c(rep("A", nrow(x2) / 2), rep("B", nrow(x2) / 2))}}\label{z---crepa-nrowx2-2-repb-nrowx2-2}

\subsection{\texorpdfstring{{[}80{]} ``fit\_anova \textless{}- aov(x2
\textasciitilde{} z, data = data.frame(x2 = x2{[}, 1{]},
z))''}{{[}80{]} fit\_anova \textless{}- aov(x2 \textasciitilde{} z, data = data.frame(x2 = x2{[}, 1{]}, z))}}\label{fit_anova---aovx2-z-data-data.framex2-x2-1-z}

\subsection{\texorpdfstring{{[}81{]}
``summary(fit\_anova)''}{{[}81{]} summary(fit\_anova)}}\label{summaryfit_anova}

```

So, let's say we've come to our script wanting to extract the code to
produce one of the results \texttt{fit.sqrt.A}, which is an analysis
that is relevant to some product. Not only do we want to double check
the results, we also want to use the code again for another purpose,
such as creating a plot of the patterns supported by the test. Manually
tracing through our code for all the variables used in the test and
finding all of the lines that were used to prepare them for the analysis
would be annoying and difficult, especially given the fact that we have
used ``x'' as a prefix for multiple unrelated objects in the script.
Instead, we can easily do this automatically with \texttt{Rclean}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{clean}\NormalTok{(script, }\StringTok{"fit_sqrt_A"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## x <- 1:100
## x <- log(x)
## x <- x * 2
## x <- lapply(x, rep, times = 4)
## x <- do.call(cbind, x)
## x <- x * 2
## colnames(x) <- paste0("X", seq_len(ncol(x)))
## rownames(x) <- LETTERS[seq_len(nrow(x))]
## x <- t(x)
## x[, "A"] <- sqrt(x[, "A"])
## for (i in seq_along(colnames(x))) {
##   set.seed(17)
##   x[, i] <- x[, i] + runif(length(x[, i]), -1, 1)
## }
## x[, 1] <- x[, 1] * 2 + 10
## x[, 2] <- x[, 1] + x[, 2]
## x[, "A"] <- x[, "A"] * 2
## x <- data.frame(x)
## fit_sqrt_A <- lm(I(sqrt(A)) ~ B, data = x)
\end{verbatim}

As you can see, \texttt{Rclean} has picked through the tangled bits of
code and found the minimal set of lines relevant to our object of
interest. This code can now be visually inspected to adapt the original
code or ported to a new, ``refactored'' script.

\subsection{Software Availability}\label{software-availability}

The software is currently hosted on Github, and we recommend using the
\texttt{devtools} library to install directly from the repository
(\url{https://github.com/ROpenSci/Rclean}). The package is open-source
and welcomes contributions. Please visit the repository page to report
issues, request features or provide other feedback.

\section{Discussion}\label{discussion}

We have created \texttt{Rclean} to provide a simple, easy to use tool
for scientists who would like help refactoring code. Using
\texttt{Rclean} the code necessary to produce a specified result (e.g.,
an object stored in memory or a table or figure written to disk) can be
easily and \emph{reliably} isolated even when tangled with code for
other results. This functionality is enabled by graph analysis of data
provenance collected from the user's script. This is likely to be a
broadly useful tool as statistical programming becomes more common
across the sciences. Tools, such as this, that make it easier to produce
transparent, accessible code will be an important aid for improving
scientific reproducibility.

Although the workflow described is impactful, we see promise in
extending it to interface with other clean code and reproducibility
tools. One example is the \texttt{reprex} package, which provides a
simple API for sharing reproducible examples. Another possibility is to
help transtion scripts to function, package and workflow creation and
refactoring via toolboxes like \texttt{drake}. \texttt{Rclean} could
provide a reliable way to extract parts of a larger script that would be
piped to a simplified reproducible example, in the case of
\texttt{reprex}, or, since it can isolate the code from inputs to one or
more outputs, be used to extract all of the components needed to write
one or more functions that would be a part of a package or workflow, as
is the goal of \texttt{drake}.

To conclude, we hope that \texttt{Rclean} makes writing scientific
software easier for the R community. We look forward to feedback and
help with extending its application, particularly in the area of
reproducibility, such as using code cleaning in the creation of more
robust capsules (Pasquier et al. 2018). To get involved, report bugs,
suggest features, please visit the project page.

\section{Acknowledgments}\label{acknowledgments}

This work was improved by discussions with ecologists at Harvard Forest
and through the helpful review provided by the ROpenSci community,
particularly Anna Krystalli, Will Landau and Clemens Schmid. Much of the
work was funded by US National Science Foundation grant SSI-1450277 for
applications of End-to-End Data Provenance.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\hypertarget{ref-Baker2016}{}
Baker, Monya. 2016. ``1,500 Scientists Lift the Lid on
Reproducibility.'' \emph{Nature} 533: 452--54.
doi:\href{https://doi.org/10.1038/533452a}{10.1038/533452a}.

\hypertarget{ref-Carata2014}{}
Carata, Lucian, Sherif Akoush, Nikilesh Balakrishnan, Thomas Bytheway,
Ripduman Sohan, Margo Seltzer, and Andy Hopper. 2014. ``A Primer on
Provenance.'' \emph{Queue} 12 (3). ACM: 10--23.
doi:\href{https://doi.org/10.1145/2602649.2602651}{10.1145/2602649.2602651}.

\hypertarget{ref-Pasquier2018}{}
Pasquier, Thomas, Matthew K. Lau, Xueyuan Han, Elizabeth Fong, Barbara
S. Lerner, Emery R. Boose, Merce Crosas, Aaron M. Ellison, and Margo
Seltzer. 2018. ``Sharing and Preserving Computational Analyses for
Posterity with encapsulator.'' \emph{Comput. Sci. Eng.} 20 (4): 111--24.
doi:\href{https://doi.org/10.1109/MCSE.2018.042781334}{10.1109/MCSE.2018.042781334}.

\hypertarget{ref-Pasquier2017}{}
Pasquier, Thomas, Matthew K. Lau, Ana Trisovic, Emery R. Boose, Ben
Couturier, MercÃ¨ Crosas, Aaron M. Ellison, Valerie Gibson, Chris R.
Jones, and Margo Seltzer. 2017. ``If these data could talk.'' \emph{Sci.
Data} 4 (September): 170114.
doi:\href{https://doi.org/10.1038/sdata.2017.114}{10.1038/sdata.2017.114}.

\hypertarget{ref-Peng2011}{}
Peng, Roger D, B. Hanson, A. Sugden, B. Alberts, R. D. Peng, F.
Dominici, S. L. Zeger, et al. 2011. ``Reproducible research in
computational science.'' \emph{Science} 334 (6060). American Association
for the Advancement of Science: 1226--7.
doi:\href{https://doi.org/10.1126/science.1213847}{10.1126/science.1213847}.

\hypertarget{ref-Stodden2018}{}
Stodden, Victoria, Jennifer Seiler, and Zhaokun Ma. 2018. ``An empirical
analysis of journal policy effectiveness for computational
reproducibility.'' \emph{Proc. Natl. Acad. Sci. U. S. A.} 115 (11).
National Academy of Sciences: 2584--9.
doi:\href{https://doi.org/10.1073/pnas.1708290115}{10.1073/pnas.1708290115}.

\hypertarget{ref-Visser2015}{}
Visser, Marco D., Sean M. McMahon, Cory Merow, Philip M. Dixon, Sydne
Record, and Eelke Jongejans. 2015. ``Speeding Up Ecological and
Evolutionary Computations in R; Essentials of High Performance Computing
for Biologists.'' Edited by Francis Ouellette. \emph{PLOS Comput. Biol.}
11 (3). Springer-Verlag: e1004140.
doi:\href{https://doi.org/10.1371/journal.pcbi.1004140}{10.1371/journal.pcbi.1004140}.

\end{document}
